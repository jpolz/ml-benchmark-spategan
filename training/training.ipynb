{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1152ffc7-1f78-46e1-b6c9-b65680b9abac",
   "metadata": {},
   "source": [
    "In this notebook, we will train an RCM emulator from scratch using PyTorch and the CORDEX Benchmark dataset (for more information on the data, see the notebooks in `../data`).\n",
    "\n",
    "The workflow is as follows:\n",
    "\n",
    "- Train the RCM emulator under a either of the two training configurations available.\n",
    "- Evaluate it on a subset of the same RCM simulations used for training.\n",
    "- Compute predictions for the different evalaution experiments included in the benchmark.\n",
    "\n",
    "We have kept the set of dependencies minimal. For further details, see the README or `../requirements`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2027acb-bde4-4a08-aefa-1e8b68b6fe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "import sys\n",
    "sys.path.append('../evaluation')\n",
    "import indices, diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8792cfd5-d31a-4d80-98ff-cb28d05fd384",
   "metadata": {},
   "source": [
    "First, we define two functions to facilitate plotting of the generated data, including the Power Spectral Density (PSD), which will be used to evaluate the RCM emulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e90561-1f45-4450-b4b1-168890723ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_map(data, var_name, domain, vmin, vmax,\n",
    "                  fig_title='', figsize=(8,8), cmap='viridis'):\n",
    "    \n",
    "    central_longitude = 180 if domain == 'NZ' else 0 if domain == 'ALPS' else None\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    ax = plt.axes(projection=ccrs.PlateCarree(central_longitude=central_longitude))\n",
    "\n",
    "    if (domain == 'NZ') or (domain == 'SA'):\n",
    "        data[var_name].plot(ax=ax, transform=ccrs.PlateCarree(),\n",
    "                            vmin=vmin, vmax=vmax,\n",
    "                            cmap=cmap)\n",
    "    elif domain == 'ALPS':\n",
    "        cs = ax.pcolormesh(data[var_name]['lon'], data[var_name]['lat'],\n",
    "                           data[var_name],\n",
    "                           transform=ccrs.PlateCarree(),\n",
    "                           vmin=vmin, vmax=vmax,\n",
    "                           cmap=cmap)\n",
    "    \n",
    "    ax.coastlines(resolution='10m')\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "    \n",
    "    plt.title(fig_title)\n",
    "    plt.show()\n",
    "\n",
    "def plot_psd(psd_target, psd_pred):\n",
    "    plt.loglog(psd_target.wavenumber, psd_target, label=\"Target\")\n",
    "    plt.loglog(psd_pred.wavenumber, psd_pred, label=\"Prediction\")\n",
    "    plt.xlabel(\"Wavenumber\")\n",
    "    plt.title(\"Power Spectral Density\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3c04c4-a9ff-4519-bc33-1514a91a6a4d",
   "metadata": {},
   "source": [
    "We define the PATHs for the data (which must match those used when downloading the benchmark dataset) and for saving the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be2ede5-bece-4a03-b160-3c28fc38a34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/bg/fast/aihydromet/cordexbench/'\n",
    "\n",
    "MODELS_PATH = './models'\n",
    "os.makedirs(MODELS_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4d11e7-aa60-465a-97ae-9edaa96e0e2f",
   "metadata": {},
   "source": [
    "For this example, we select the NZ domain. (The code, as written, is also valid for the ALPS domain.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268c1e57-222c-4c37-8938-90ff329fb527",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = 'SA'\n",
    "training_experiment = 'ESD_pseudo_reality'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a465326d-51d5-47f3-9aa6-f6f7e7cc9696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the period\n",
    "if training_experiment == 'ESD_pseudo_reality':\n",
    "    period_training = '1961-1980'\n",
    "elif training_experiment == 'Emulator_hist_future':\n",
    "    period_training = '1961-1980_2080-2099'\n",
    "else:\n",
    "    raise ValueError('Provide a valid date')\n",
    "\n",
    "# Set the GCM\n",
    "if domain == 'ALPS':\n",
    "    gcm_name = 'CNRM-CM5'\n",
    "elif (domain == 'NZ') or (domain == 'SA'):\n",
    "    gcm_name = 'ACCESS-CM2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0f1733-fe86-42d3-88de-221e62019f52",
   "metadata": {},
   "source": [
    "We load the predictor. For more details on the properties of this dataset (and the predictand), see `./data/experiments.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3d3973-baba-4e40-bcf2-0e6ce97a5d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_filename = f'{DATA_PATH}/{domain}/{domain}_domain/train/{training_experiment}/predictors/{gcm_name}_{period_training}.nc'\n",
    "predictor = xr.open_dataset(predictor_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c7bf0b-19a5-422b-9cbe-88367acb8ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if domain == 'SA':\n",
    "    predictor = predictor.drop_vars('time_bnds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655390e9-c583-48f6-9b54-e3eadd91e105",
   "metadata": {},
   "source": [
    "As predictands, the benchmark provides both daily maximum temperature (`tasmax`) and daily accumulated precipitation (`pr`). For this tutorial, we focus on maximum temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75a5028-174f-44c3-96ec-c5b5e99c8e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_target = 'tasmax'\n",
    "\n",
    "predictand_filename = f'{DATA_PATH}/{domain}/{domain}_domain/train/{training_experiment}/target/pr_tasmax_{gcm_name}_{period_training}.nc'\n",
    "predictand = xr.open_dataset(predictand_filename)\n",
    "predictand = predictand[[var_target]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d955269-c34b-4938-bb9d-a59a25956413",
   "metadata": {},
   "source": [
    "Unfortunately, the benchmark does not yet provide ground-truth data (target RCM simulations) for the evaluation experiments. Therefore, to get an initial sense of the trained DL emulator’ performance, we generate a test set from the data used for training. Evaluating on this test set provides an indication of how well the emulator is performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3abca5-8dbf-4e74-9a45-fe82a1cc7ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_experiment == 'ESD_pseudo_reality':\n",
    "    years_train = list(range(1961, 1975))\n",
    "    years_test = list(range(1975, 1980+1))\n",
    "elif training_experiment == 'Emulator_hist_fut':\n",
    "    years_train = list(range(1961, 1980+1)) + list(range(2080, 2090))\n",
    "    years_test = list(range(2090, 2099+1))\n",
    "\n",
    "x_train = predictor.sel(time=np.isin(predictor['time'].dt.year, years_train))\n",
    "y_train = predictand.sel(time=np.isin(predictand['time'].dt.year, years_train))\n",
    "\n",
    "x_test = predictor.sel(time=np.isin(predictor['time'].dt.year, years_test))\n",
    "y_test = predictand.sel(time=np.isin(predictand['time'].dt.year, years_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eadf833-5c4a-4baf-8ded-48c6034b2539",
   "metadata": {},
   "source": [
    "As is typical in the DL community, we standardize the training predictors to have a mean of zero and a standard deviation of one. Importantly, the test predictors are standardized using the mean and standard deviation from the training set. This approach ensures that the input data scale remains consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f555b31-44d1-499b-aec8-a440217cc635",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_train = x_train.mean('time')\n",
    "std_train = x_train.std('time')\n",
    "\n",
    "x_train_stand = (x_train - mean_train) / std_train\n",
    "x_test_stand = (x_test - mean_train) / std_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f9c208-a1c1-4749-bd9f-e071a19e3aec",
   "metadata": {},
   "source": [
    "In this tutorial, we train a DL model with a final fully connected layer, so the model output is a flattened vector (which we will later reshape to its original 2D dimensions). Therefore, the target data must also be flattened for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e62163-de39-459f-91d5-ef32cc9a444f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if domain == 'ALPS':\n",
    "    spatial_dims = ('x', 'y')\n",
    "elif (domain == 'NZ') or (domain == 'SA'):\n",
    "    spatial_dims = ('lat', 'lon')\n",
    "\n",
    "y_train_stack = y_train.stack(gridpoint=spatial_dims)\n",
    "y_test_stack = y_test.stack(gridpoint=spatial_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01e4f61-cd1a-4a2c-a413-e32416b19ec1",
   "metadata": {},
   "source": [
    "Finally, we extract the raw data from the `xarray.Dataset` as a NumPy array and convert it to a PyTorch tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929fdf21-a965-4e5a-9a2f-3d0b87cb6467",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_stand_array = torch.from_numpy(x_train_stand.to_array().transpose(\"time\", \"variable\", \"lat\", \"lon\").values)\n",
    "y_train_stack_array = torch.from_numpy(y_train_stack.to_array()[0, :].values)\n",
    "\n",
    "x_test_stand_array = torch.from_numpy(x_test_stand.to_array().transpose(\"time\", \"variable\", \"lat\", \"lon\").values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6eb9d24-7171-4f16-a56e-7c44b53bf93d",
   "metadata": {},
   "source": [
    "We define two separate Dataset objects: one for training (outputting both inputs and targets) and one for making predictions (outputting only the inputs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de19601-5d2b-4afc-b546-d1f52ebc40a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmulationTrainingDataset(Dataset):\n",
    "    def __init__(self, x_data, y_data):\n",
    "        if not isinstance(x_data, torch.Tensor):\n",
    "            x_data = torch.tensor(x_data)\n",
    "        if not isinstance(y_data, torch.Tensor):\n",
    "            y_data = torch.tensor(y_data)\n",
    "        self.x_data, self.y_data = x_data, y_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_sample, y_sample = self.x_data[idx, :], self.y_data[idx, :]\n",
    "        return x_sample, y_sample\n",
    "\n",
    "class EmulationTestDataset(Dataset):\n",
    "    def __init__(self, x_data):\n",
    "        if not isinstance(x_data, torch.Tensor):\n",
    "            x_data = torch.tensor(x_data)\n",
    "        self.x_data = x_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_data[idx, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98100760-9bf9-4b11-b096-dcc79b3a0982",
   "metadata": {},
   "source": [
    "We set training Dataset and DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055eeb0d-aed4-483f-b03c-59555ae5297f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_training = EmulationTrainingDataset(x_data=x_train_stand_array, y_data=y_train_stack_array)\n",
    "dataloader_train = DataLoader(dataset=dataset_training,\n",
    "                              batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55236b17-7607-4899-9216-18d8e33fbe7a",
   "metadata": {},
   "source": [
    "Now we define the DL architecture to use as our RCM emulator. In this example, we use DeepESD [1], a model previously applied for both observational statistical downscaling and RCM emulation [2]. The architecture consists of three convolutional layers followed by a final dense layer. Despite its simplicity, DeepESD has demonstrated strong performance and has been successfully applied across various domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d971290c-fadf-4997-9cfe-499c4a776f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepESD(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, x_shape: tuple, y_shape: tuple,\n",
    "                 filters_last_conv: int):\n",
    "\n",
    "        super(DeepESD, self).__init__()\n",
    "\n",
    "        self.x_shape = x_shape\n",
    "        self.y_shape = y_shape\n",
    "        self.filters_last_conv = filters_last_conv\n",
    "\n",
    "        self.conv_1 = torch.nn.Conv2d(in_channels=self.x_shape[1],\n",
    "                                      out_channels=50,\n",
    "                                      kernel_size=3,\n",
    "                                      padding=1)\n",
    "\n",
    "        self.conv_2 = torch.nn.Conv2d(in_channels=50,\n",
    "                                      out_channels=25,\n",
    "                                      kernel_size=3,\n",
    "                                      padding=1)\n",
    "\n",
    "        self.conv_3 = torch.nn.Conv2d(in_channels=25,\n",
    "                                      out_channels=self.filters_last_conv,\n",
    "                                      kernel_size=3,\n",
    "                                      padding=1)\n",
    "\n",
    "        self.out = torch.nn.Linear(in_features=\\\n",
    "                                       self.x_shape[2] * self.x_shape[3] * self.filters_last_conv,\n",
    "                                       out_features=self.y_shape[1])\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        x = self.conv_1(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x = self.conv_2(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x = self.conv_3(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "\n",
    "        out = self.out(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96df7d19-7fc3-4d85-bd38-d74c9eadc61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepESD(x_shape=x_train_stand_array.shape,\n",
    "                y_shape=y_train_stack_array.shape,\n",
    "                filters_last_conv=1)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e5344e-604c-4532-9b5f-5d622a66ac8f",
   "metadata": {},
   "source": [
    "We use Mean Squared Error (MSE) as the loss function, which is commonly employed for temperature-related tasks. If working with precipitation instead, we recommend exploring alternative loss functions, since MSE is not well-suited for learning distributions with strong skewness, such as precipitation. For further details, see [3]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ec5254-97d2-43f9-8a5e-cf262ea4a3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b175275e-23bf-4db1-885d-8061f527475c",
   "metadata": {},
   "source": [
    "We set the hyperparameters that control how the model weights are learned and select the available device for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade7fd40-1491-4593-a709-abf16910d9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54af4a97-5658-4e5f-a164-e5150024d125",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22e4f56-f70a-4f52-8d08-2a0cd49b3b3e",
   "metadata": {},
   "source": [
    "Below, we define the `for` loop to train the model. While there are simpler ways to train DL models within the PyTorch ecosystem, this implementation is useful to gain a clear understanding of the entire training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084815ad-1447-4215-a77f-45e89da02492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store the loss per epoch\n",
    "loss_train = []\n",
    "\n",
    "# Iterate over the epochs\n",
    "for epoch in range(num_epochs):\n",
    "    # Define a variable to accumulate the cost function value per epoch\n",
    "    epoch_loss = 0.0 \n",
    "    \n",
    "    # Iterate over the batches\n",
    "    for idx, (batch_x_data, batch_y_data) in enumerate(dataloader_train):\n",
    "        batch_size = batch_x_data.shape[0]\n",
    "\n",
    "        # Move data to the device\n",
    "        batch_x_data = batch_x_data.to(device)\n",
    "        batch_y_data = batch_y_data.to(device)\n",
    "        \n",
    "        # Zero the gradients since by default they accumulate\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute the forward pass\n",
    "        outputs = model(batch_x_data)  # PyTorch calls model.forward() internally\n",
    "\n",
    "        # Compute the cost function\n",
    "        loss_batch = loss_function(outputs, batch_y_data)\n",
    "        epoch_loss += batch_size * loss_batch.item()\n",
    "\n",
    "        # Compute the gradients (backward pass)\n",
    "        loss_batch.backward()\n",
    "\n",
    "        # Update the model weights\n",
    "        optimizer.step()\n",
    "\n",
    "    # Calculate the loss for the epoch\n",
    "    epoch_loss = epoch_loss / len(dataset_training)\n",
    "    loss_train.append(epoch_loss)\n",
    "    \n",
    "    # Print every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.6f}')\n",
    "\n",
    "# At the end of training, save the model weights\n",
    "model_name = 'model.pt'\n",
    "torch.save(model.state_dict(), f'{MODELS_PATH}/{model_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12cb102-9463-4f83-948c-9bd755a09edd",
   "metadata": {},
   "source": [
    "Once the model is trained, we can compute predictions on the test set defined at the beginning of the notebook. To do this, we create a DataLoader for the test data and pass it through the trained model, obtaining predictions for the entire test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7cf18e-58b9-49eb-bee6-9f4593fe76c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = EmulationTestDataset(x_data=x_test_stand_array)\n",
    "test_dataloader = DataLoader(dataset=dataset_test,\n",
    "                             batch_size=32, shuffle=False)\n",
    "\n",
    "# Compute predictions\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch_x in test_dataloader:\n",
    "        batch_x = batch_x.to(next(model.parameters()).device)\n",
    "        outputs = model(batch_x)\n",
    "        predictions.append(outputs.cpu().numpy())\n",
    "\n",
    "# Concatenate all batches into one array\n",
    "predictions = np.concatenate(predictions, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b496f8-0e99-4a70-99e9-154078f09464",
   "metadata": {},
   "source": [
    "The DeepESD model outputs a flattened version of the RCM simulations, so we need to reshape it back to 2D. This is done in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b32aec-30a7-4550-b6b0-20c201d5f391",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_stack = y_test_stack.copy(deep=True)\n",
    "y_pred_stack[var_target].values = predictions\n",
    "y_pred = y_pred_stack.unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b994910c-4485-4c17-9225-4aa89315adb8",
   "metadata": {},
   "source": [
    "Once the predictions for the test set have been computed, we can evaluate them using several metrics introduced in the class slides. Specifically, we compute the following metrics:\n",
    "\n",
    "- Root Mean Squared Error (RMSE)\n",
    "- Bias of the mean\n",
    "- Bias of the 98th percentile\n",
    "- Ratio of standard deviations\n",
    "- Power Spectral Density (PSD)\n",
    "\n",
    "The implementations of these metrics can be found in `../evaluation/metrics.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73931acc-478c-45d7-82cf-af93035b60d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = diagnostics.rmse(x0=y_test, x1=y_pred,\n",
    "                        var=var_target, dim='time')\n",
    "print(f'Mean RMSE: {rmse[var_target].mean().values.item()}')\n",
    "\n",
    "plot_data_map(data=rmse, var_name=var_target, domain=domain, vmin=0, vmax=5,\n",
    "              fig_title='RMSE', cmap='Reds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c0cdb5-a8c7-40d4-b2a0-32e7c23f298b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_mean = diagnostics.bias_index(x0=y_test, x1=y_pred,\n",
    "                                   index_fn=indices.mean,\n",
    "                                   var=var_target)\n",
    "print(f'Mean Bias Mean: {bias_mean[var_target].mean().values.item()}')\n",
    "\n",
    "plot_data_map(data=bias_mean, var_name=var_target, domain=domain, vmin=-2, vmax=2,\n",
    "              fig_title='Bias Mean', cmap='RdBu_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c285efa1-faf7-451d-aaab-04971c897e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_p98 = diagnostics.bias_index(x0=y_test, x1=y_pred,\n",
    "                                  index_fn=indices.quantile,\n",
    "                                  q=0.98,\n",
    "                                  var=var_target)\n",
    "print(f'Mean Bias 98th percentile: {bias_p98[var_target].mean().values.item()}')\n",
    "\n",
    "plot_data_map(data=bias_p98, var_name=var_target, domain=domain, vmin=-2, vmax=2,\n",
    "              fig_title='Bias 98th Percentile', cmap='RdBu_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17d4cad-1fb5-40f5-a2b3-0a0c86db17bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "psd_target, psd_pred = diagnostics.psd(x0=y_test, x1=y_pred,\n",
    "                                       var=var_target)\n",
    "plot_psd(psd_target=psd_target, psd_pred=psd_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c6ff6f-f3f5-4e1e-93df-6263f441ea75",
   "metadata": {},
   "source": [
    "For visualization purposes, we also plot the RCM simulation alongside the emulator’s prediction for a specific day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd455e61-5150-47ea-9789-308ce1e13cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_idx = 0\n",
    "\n",
    "plot_data_map(data=y_test.isel(time=time_idx), var_name=var_target, domain=domain,\n",
    "              vmin=290, vmax=310,\n",
    "              fig_title=f'RCM simulation for {time_idx} time step', figsize=(8,8), cmap='RdBu_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeb79aa-a985-4f96-a0f2-d26b050b2a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_map(data=y_pred.isel(time=0), var_name=var_target, domain=domain,\n",
    "              vmin=290, vmax=310,\n",
    "              fig_title=f'RCM Emulator prediction for {time_idx} time step', figsize=(8,8), cmap='RdBu_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f6e952-3a8c-48a7-bf8a-6227d7a3d359",
   "metadata": {},
   "source": [
    "Once the RCM emulator is trained, we can compute predictions for the different evaluation experiments. For details on how the data is loaded, please refer to `../data/experiments.ipynb`. We set the evaluation experiment in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b9ec21-1dbc-4726-8162-bcaec9d9a971",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_experiment = 'PP_cross_validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b294ac-fa1e-4764-ac97-3bd6f93224a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment settings\n",
    "evaluation_experiment_settings = {\n",
    "    \"ESD_pseudo_reality\": {\n",
    "        \"PP_cross_validation\": (\"historical\", \"perfect\", True),\n",
    "        \"Imperfect_cross_validation\": (\"historical\", \"imperfect\", True),\n",
    "        \"Extrapolation_perfect\": (\"mid_end_century\", \"perfect\", True),\n",
    "        \"Extrapolation_imperfect\": (\"mid_end_century\", \"imperfect\", True),\n",
    "    },\n",
    "    \"Emulator_hist_future\": {\n",
    "        \"PP_cross_validation\": (\"historical\", \"perfect\", True),\n",
    "        \"Imperfect_cross_validation\": (\"historical\", \"imperfect\", True),\n",
    "        \"Extrapolation_perfect\": (\"mid_end_century\", \"perfect\", True),\n",
    "        \"Extrapolation_perfect_hard\": (\"mid_end_century\", \"perfect\", False),\n",
    "        \"Extrapolation_imperfect_hard\": (\"mid_end_century\", \"imperfect\", False),\n",
    "    },\n",
    "}\n",
    "\n",
    "# Get settings for the chosen experiments\n",
    "period, mode, same_gcm_as_train = evaluation_experiment_settings[training_experiment][evaluation_experiment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d80e62-a951-4852-b09a-d0af28d73e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map periods to dates\n",
    "period_dates = {\n",
    "    \"historical\": \"1981-2000\",\n",
    "    \"mid_century\": \"2041-2060\",\n",
    "    \"end_century\": \"2080-2099\",\n",
    "    \"mid_end_century\": [\"2041-2060\", \"2080-2099\"],\n",
    "}\n",
    "\n",
    "period_date = period_dates[period]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385f59e6-9fb6-4631-9391-e44a01e3c27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCM selection by domain and training setup\n",
    "gcm_train = {\"NZ\": \"ACCESS-CM2\", \"ALPS\": \"CNRM-CM5\", \"SA\": \"ACCESS-CM2\"}\n",
    "gcm_eval = {\"NZ\": \"EC-Earth3\", \"ALPS\": \"MPI-ESM-LR\", \"SA\": \"NorESM2-MM\"}\n",
    "\n",
    "if same_gcm_as_train:\n",
    "    gcm_name = gcm_train[domain]\n",
    "else:\n",
    "    gcm_name = gcm_eval[domain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a268a6-6fbb-4307-a84f-748204fc16a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predictor\n",
    "if period == 'mid_end_century':\n",
    "    predictor_evaluation_filename_mid_century = f'{DATA_PATH}/{domain}/{domain}_domain/test/mid_century/predictors/{mode}/{gcm_name}_2041-2060.nc'\n",
    "    predictor_evaluation_mid = xr.open_dataset(predictor_evaluation_filename_mid_century)\n",
    "    \n",
    "    predictor_evaluation_filename_end_century = f'{DATA_PATH}/{domain}/{domain}_domain/test/end_century/predictors/{mode}/{gcm_name}_2080-2099.nc'\n",
    "    predictor_evaluation_end = xr.open_dataset(predictor_evaluation_filename_end_century)\n",
    "\n",
    "    predictor_evaluation = xr.merge([predictor_evaluation_mid, predictor_evaluation_end])\n",
    "else:\n",
    "    predictor_evaluation_filename = f'{DATA_PATH}/{domain}/{domain}_domain/test/{period}/predictors/{mode}/{gcm_name}_{period_date}.nc'\n",
    "    predictor_evaluation = xr.open_dataset(predictor_evaluation_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fdf0bb-5350-437f-ae28-0890c4659230",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'time_bnds' in list(predictor_evaluation.data_vars):\n",
    "    predictor_evaluation = predictor_evaluation.drop_vars('time_bnds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7608f2-13e1-418f-a0b8-45848c29db45",
   "metadata": {},
   "source": [
    "Before passing the predictors to the DL model, we standardize them in the same way as for the previous test partition and prepare the data for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7980512-7a62-4c18-bb49-46771307aee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_eval_stand = (predictor_evaluation - mean_train) / std_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9229a79f-c9d4-4147-8085-589e45208114",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_eval_stand_array = torch.from_numpy(x_eval_stand.to_array().transpose(\"time\", \"variable\", \"lat\", \"lon\").values)\n",
    "\n",
    "dataset_eval = EmulationTestDataset(x_data=x_eval_stand_array)\n",
    "eval_dataloader = DataLoader(dataset=dataset_eval,\n",
    "                             batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b17caf5-42cb-47d7-ac85-80d76265d221",
   "metadata": {},
   "source": [
    "Finally, we compute the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cc3cfe-6878-433c-abcf-6b448082949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predictions\n",
    "model.eval()\n",
    "\n",
    "predictions_eval = []\n",
    "with torch.no_grad():\n",
    "    for batch_x in eval_dataloader:\n",
    "        batch_x = batch_x.to(next(model.parameters()).device)\n",
    "        outputs = model(batch_x)\n",
    "        predictions_eval.append(outputs.cpu().numpy())\n",
    "\n",
    "# Concatenate all batches into one array\n",
    "predictions_eval = np.concatenate(predictions_eval, axis=0)\n",
    "\n",
    "# Reshape\n",
    "y_eval_pred_stack = y_test_stack.copy(deep=True)\n",
    "y_eval_pred_stack = y_eval_pred_stack.reindex(time=x_eval_stand.time, method=None)\n",
    "\n",
    "y_eval_pred_stack[var_target].values = predictions_eval\n",
    "y_eval_pred = y_eval_pred_stack.unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e1c282-2392-4981-89b8-64daea121f4a",
   "metadata": {},
   "source": [
    "We can now visualize the simulations generated by the RCM emulator for the selected evaluation experiment. In this example, we plot the maximum temperature for a single day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d88252-5d18-49b3-a091-e716723d70f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_idx = 0\n",
    "\n",
    "plot_data_map(data=y_eval_pred.isel(time=0), var_name=var_target, domain=domain,\n",
    "              vmin=270, vmax=305,\n",
    "              fig_title=f'RCM simulation for {time_idx} time step ({evaluation_experiment})',\n",
    "              figsize=(8,8), cmap='RdBu_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9d71d8-aeb2-4cf6-8bd7-8205617a9166",
   "metadata": {},
   "source": [
    "**References**\n",
    "\n",
    "[1] Baño-Medina, J., Manzanas, R., Cimadevilla, E., Fernández, J., González-Abad, J., Cofiño, A. S., & Gutiérrez, J. M. (2022). **Downscaling multi-model climate projection ensembles with deep learning (DeepESD): contribution to CORDEX EUR-44**. Geoscientific Model Development Discussions, 2022, 1-14.\n",
    "\n",
    "[2] Baño-Medina, J., Iturbide, M., Fernández, J., & Gutiérrez, J. M. (2024). **Transferability and explainability of deep learning emulators for regional climate model projections: Perspectives for future applications**. Artificial Intelligence for the Earth Systems, 3(4), e230099.\n",
    "\n",
    "[3] González-Abad, J., & Gutiérrez, J. M. (2025). **Are Deep Learning Methods Suitable for Downscaling Global Climate Projections? An Intercomparison for Temperature and Precipitation over Spain**. Artificial Intelligence for the Earth Systems, 4(4), 240121."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-benchmark-spategan (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
