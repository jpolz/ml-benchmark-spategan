{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3b083b3",
   "metadata": {},
   "source": [
    "## V2 of the training notebook for GANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be0caf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchinfo import summary\n",
    "\n",
    "from ml_benchmark_spategan.config import config\n",
    "from ml_benchmark_spategan.dataloader import dataloader\n",
    "from ml_benchmark_spategan.model.spagan2d import Generator, Discriminator, train_gan_step\n",
    "from ml_benchmark_spategan.visualization.plot_train import plot_adversarial_losses, plot_predictions, plot_diagnostic_history\n",
    "from ml_benchmark_spategan.utils.denormalize import predictions_to_xarray\n",
    "\n",
    "from diffusers import UNet2DModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Import diagnostics\n",
    "import sys\n",
    "sys.path.append('../evaluation')\n",
    "import diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f034de71",
   "metadata": {},
   "source": [
    "### Load configuration and initialize run directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b4c769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find project base directory\n",
    "project_base = pathlib.Path(os.getcwd()).parent\n",
    "# load configuration\n",
    "cf = config.set_up_run(project_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5fe16b",
   "metadata": {},
   "source": [
    "### Build dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9421b773",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train, test_dataloader, cf, norm_params = dataloader.build_dataloaders(cf)\n",
    "# dataloader_train, test_dataloader = dataloader.build_dummy_dataloaders()\n",
    "# update cf in run directory\n",
    "cf.save()\n",
    "# describe shapes of data\n",
    "print(\"Training data shapes:\")\n",
    "x_shape, y_shape = dataloader_train.dataset._get_shapes()\n",
    "print(f\"  x: {x_shape}\")\n",
    "print(f\"  y: {y_shape}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5855bdd6",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "class denorm_y:\n",
    "    def __init__(self, y_min, y_max):\n",
    "        self.y_min = y_min\n",
    "        self.y_max = y_max\n",
    "\n",
    "    def __call__(self, y_norm):\n",
    "        y_denorm = (y_norm + 1) / 2 * (self.y_max - self.y_min) + self.y_min\n",
    "        return y_denorm\n",
    "\n",
    "denorm_y_fn = denorm_y(cf.data.y_min, cf.data.y_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c998347",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "match cf.model.generator_architecture:\n",
    "    case \"spategan\":\n",
    "        print(\"Using SpaGAN architecture\")\n",
    "        # Initialize models\n",
    "        generator = Generator(cf.model).to(device)\n",
    "\n",
    "        # Print model summaries\n",
    "        print(\"Generator architecture:\")\n",
    "        summary(generator, input_size=(1, 15, 16, 16))\n",
    "    case \"diffusion_unet\":\n",
    "        print(\"Using Diffusion UNet architecture\")\n",
    "        generator = UNet2DModel(\n",
    "                sample_size=(128, 128),\n",
    "                in_channels=15+1,  # +1 == noise\n",
    "                out_channels=1,\n",
    "                layers_per_block=2,\n",
    "                block_out_channels=(64, 128, 128, 256),\n",
    "                down_block_types=(\"DownBlock2D\", \"DownBlock2D\", \"DownBlock2D\", \"AttnDownBlock2D\"),\n",
    "                up_block_types=(\"UpBlock2D\", \"AttnUpBlock2D\", \"UpBlock2D\", \"UpBlock2D\"),\n",
    "            ).to(device)\n",
    "        print(summary(generator, input_size=[(1, 16, 128, 128), (1,)], dtypes=[torch.float32, torch.long]))\n",
    "\n",
    "\n",
    "    case _:\n",
    "        raise ValueError(f\"Invalid option: {cf.model.generator_architecture}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21503eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cf.model.discriminator_architecture == \"unet\":\n",
    "    print(\"Using UNet2DModel as discriminator\")\n",
    "    discriminator = UNet2DModel(\n",
    "                    sample_size=(128, 128),\n",
    "                    in_channels=2,  # +1 == noise\n",
    "                    out_channels=1,\n",
    "                    layers_per_block=2,\n",
    "                    block_out_channels=(32, 64, 128),\n",
    "                    down_block_types=(\"DownBlock2D\", \"DownBlock2D\", \"AttnDownBlock2D\"),\n",
    "                    up_block_types=(\"UpBlock2D\", \"AttnUpBlock2D\", \"UpBlock2D\"),\n",
    "                ).to(device)\n",
    "    print(summary(discriminator, input_size=[(1, 2, 128, 128), (1,)], dtypes=[torch.float32, torch.long]))\n",
    "elif cf.model.discriminator_architecture == \"spagan\":\n",
    "    print(\"Using SpaGAN Discriminator\")\n",
    "    discriminator = Discriminator(cf).to(device)\n",
    "    print(\"\\nDiscriminator architecture:\")\n",
    "    # Note: Discriminator takes (high_res_target, low_res_input)\n",
    "    print(summary(discriminator, input_size=[(1, 1, 128, 128), (1, 15, 16, 16)]))\n",
    "else:\n",
    "    raise ValueError(f\"Invalid option: {cf.model.discriminator_architecture}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41558533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move models to device\n",
    "generator = generator.to(device)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Optimizers\n",
    "gen_opt = torch.optim.AdamW(\n",
    "    generator.parameters(), \n",
    "    lr=cf.training.generator.learning_rate, \n",
    "    betas=(cf.training.generator.beta1, cf.training.generator.beta2), # todo: try with momentum\n",
    "    weight_decay=cf.training.generator.weight_decay\n",
    ")\n",
    "\n",
    "disc_opt = torch.optim.AdamW(\n",
    "    discriminator.parameters(), \n",
    "    lr=cf.training.discriminator.learning_rate, \n",
    "    betas=(cf.training.discriminator.beta1, cf.training.discriminator.beta2), # not using momentum on the disc since it can lead to instability\n",
    "    weight_decay=cf.training.discriminator.weight_decay\n",
    ")\n",
    "\n",
    "# For mixed precision training\n",
    "scaler = torch.amp.GradScaler('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d156f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN Training loop\n",
    "loss_gen_train = []\n",
    "loss_disc_train = []\n",
    "loss_gen_test = []\n",
    "\n",
    "# Store diagnostics\n",
    "diagnostic_history = {\n",
    "    'rmse': [],\n",
    "    'bias_mean': [],\n",
    "    'epochs': []\n",
    "}\n",
    "\n",
    "print(f\"Starting GAN training for {cf.training.epochs} epochs...\")\n",
    "\n",
    "# Get a fixed batch for visualization\n",
    "val_iter = iter(test_dataloader)\n",
    "x_vis, y_vis = next(val_iter)\n",
    "x_vis, y_vis = x_vis.to(device), y_vis.to(device)\n",
    "if cf.model.generator_architecture == \"diffusion_unet\":\n",
    "    x_vis = dataloader.upscale_nn(x_vis)\n",
    "    x_vis = dataloader.add_noise_channel(x_vis) # add noise to HR or LR?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(cf.training.epochs):\n",
    "    # Training phase\n",
    "    epoch_gen_losses = []\n",
    "    epoch_disc_losses = []\n",
    "    \n",
    "    for batch_idx, (x_batch, y_batch) in tqdm(enumerate(dataloader_train), total=len(dataloader_train)):\n",
    "        x_batch = x_batch.to(device)\n",
    "        x_batch_hr = dataloader.upscale_nn(x_batch) # move to where it is needed, if needed\n",
    "        # during training, noise channel is added during train step\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        # zero timestep, for diffusion UNET.\n",
    "        timesteps = torch.zeros([x_batch.shape[0]]).to(device) # only for diffusion unet \n",
    "        \n",
    "        # Reshape y_batch to 2D for discriminator\n",
    "        y_batch_2d = y_batch.view(-1, 1, 128, 128)\n",
    "        \n",
    "        # Use the train_gan_step function from spagan2d\n",
    "        gen_loss, disc_loss = train_gan_step(\n",
    "            config=cf,\n",
    "            input_image=x_batch,\n",
    "            input_image_hr=x_batch_hr,\n",
    "            target=y_batch_2d,\n",
    "            step=epoch * len(dataloader_train) + batch_idx,\n",
    "            discriminator=discriminator,\n",
    "            generator=generator,\n",
    "            gen_opt=gen_opt,\n",
    "            disc_opt=disc_opt,\n",
    "            scaler=scaler,\n",
    "            criterion=criterion,\n",
    "            timesteps=timesteps,\n",
    "            loss_weights=cf.training.loss_weights,\n",
    "            condition_separate_channels=True,\n",
    "        )\n",
    "        \n",
    "        epoch_gen_losses.append(gen_loss)\n",
    "        epoch_disc_losses.append(disc_loss)\n",
    "    \n",
    "    # Calculate average training losses\n",
    "    train_gen_loss = np.mean(epoch_gen_losses)\n",
    "    train_disc_loss = np.mean(epoch_disc_losses)\n",
    "    loss_gen_train.append(train_gen_loss)\n",
    "    loss_disc_train.append(train_disc_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    generator.eval()\n",
    "    test_losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_dataloader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            x_batch_hr = dataloader.upscale_nn(x_batch)\n",
    "            x_batch_hr = dataloader.add_noise_channel(x_batch_hr) # add noise to HR or LR?\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            # zero timestep, for diffusion UNET.\n",
    "            timesteps = torch.zeros([x_batch.shape[0]]).to(device)\n",
    "            \n",
    "            with torch.amp.autocast('cuda'):\n",
    "                match cf.model.generator_architecture:\n",
    "                    case \"spategan\":\n",
    "                        y_pred = generator(x_batch)\n",
    "                    case \"diffusion_unet\":\n",
    "                        y_pred = generator(x_batch_hr, timesteps).sample\n",
    "                        y_pred = torch.flatten(y_pred, start_dim=1)\n",
    "                loss = nn.L1Loss()(y_pred, y_batch)\n",
    "            \n",
    "            test_losses.append(loss.item())\n",
    "    \n",
    "    test_loss = np.mean(test_losses)\n",
    "    loss_gen_test.append(test_loss)\n",
    "    \n",
    "    # Print progress and plot\n",
    "    if (epoch + 1) % cf.logging.log_frequency == 0 or epoch == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f'Epoch {epoch+1}/{cf.training.epochs}')\n",
    "        print(f'  Generator Loss:     {train_gen_loss:.6f}')\n",
    "        print(f'  Discriminator Loss: {train_disc_loss:.6f}')\n",
    "        print(f'  Test Loss (L1):     {test_loss:.6f}')\n",
    "        \n",
    "        # Plot losses\n",
    "        plot_adversarial_losses(loss_gen_train, loss_disc_train, loss_gen_test, cf)\n",
    "    \n",
    "    # Compute diagnostics\n",
    "    if (epoch + 1) % cf.logging.diagnostic_frequency == 0:\n",
    "        print(f'  Computing diagnostics...')\n",
    "        generator.eval()\n",
    "        \n",
    "        # Collect all test predictions\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in test_dataloader:\n",
    "                x_batch = x_batch.to(device)\n",
    "                x_batch_hr = dataloader.upscale_nn(x_batch)\n",
    "                x_batch_hr = dataloader.add_noise_channel(x_batch_hr)\n",
    "                \n",
    "                timesteps = torch.zeros([x_batch.shape[0]]).to(device)\n",
    "                \n",
    "                with torch.amp.autocast('cuda'):\n",
    "                    match cf.model.generator_architecture:\n",
    "                        case \"spategan\":\n",
    "                            y_pred = generator(x_batch)\n",
    "                        case \"diffusion_unet\":\n",
    "                            y_pred = generator(x_batch_hr, timesteps).sample\n",
    "                            y_pred = torch.flatten(y_pred, start_dim=1)\n",
    "                \n",
    "                all_preds.append(y_pred.cpu())\n",
    "                all_targets.append(y_batch.cpu())\n",
    "        \n",
    "        # Concatenate all batches\n",
    "        y_pred_all = torch.cat(all_preds, dim=0)\n",
    "        y_true_all = torch.cat(all_targets, dim=0)\n",
    "        \n",
    "        # Convert to xarray with denormalization\n",
    "        pred_ds, true_ds = predictions_to_xarray(\n",
    "            y_pred_all, y_true_all, norm_params, var_name=cf.data.var_target\n",
    "        )\n",
    "        \n",
    "        # Compute diagnostics\n",
    "        rmse = diagnostics.rmse(true_ds, pred_ds, var=cf.data.var_target, dim='time')\n",
    "        bias_mean = diagnostics.bias_index(true_ds, pred_ds, \n",
    "                                          index_fn=lambda x, **kw: x[cf.data.var_target].mean('time'))\n",
    "        diagnostic_history['rmse'].append(rmse[cf.data.var_target].mean().values.item())\n",
    "        diagnostic_history['bias_mean'].append(bias_mean.mean().values.item())\n",
    "        diagnostic_history['epochs'].append(epoch + 1)\n",
    "        \n",
    "        print(f'  RMSE (spatial mean): {diagnostic_history[\"rmse\"][-1]:.4f}')\n",
    "        print(f'  Bias Mean (spatial mean): {diagnostic_history[\"bias_mean\"][-1]:.4f}')\n",
    "        plot_diagnostic_history(diagnostic_history, cf)\n",
    "        \n",
    "    # Save checkpoint\n",
    "    if (epoch + 1) % cf.logging.checkpoint_frequency == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'generator_state_dict': generator.state_dict(),\n",
    "            'discriminator_state_dict': discriminator.state_dict(),\n",
    "            'gen_optimizer_state_dict': gen_opt.state_dict(),\n",
    "            'disc_optimizer_state_dict': disc_opt.state_dict(),\n",
    "            'train_gen_loss': train_gen_loss,\n",
    "            'train_disc_loss': train_disc_loss,\n",
    "            'test_loss': test_loss,\n",
    "            'diagnostic_history': diagnostic_history,\n",
    "        }, f'{cf.logging.run_dir}/checkpoint_epoch_{epoch+1}.pt')\n",
    "        print(f'  Checkpoint saved')\n",
    "\n",
    "    if (epoch + 1) % cf.logging.map_frequency == 0:\n",
    "        # Visualize predictions with denormalization\n",
    "        g = torch.Generator(device=\"cpu\")\n",
    "        g.seed()  # uses system entropy\n",
    "        idx = torch.randint(0, x_vis.size(0), (1,), generator=g).item()\n",
    "        print(f'  Plotting sample {idx}')\n",
    "        plot_predictions(generator, x_vis, y_vis, cf, epoch + 1, device, \n",
    "                        sample_idx=idx, norm_params=norm_params)\n",
    "\n",
    "# Save final models\n",
    "torch.save({\n",
    "    'epoch': cf.training.epochs,\n",
    "    'generator_state_dict': generator.state_dict(),\n",
    "    'discriminator_state_dict': discriminator.state_dict(),\n",
    "    'gen_optimizer_state_dict': gen_opt.state_dict(),\n",
    "    'disc_optimizer_state_dict': disc_opt.state_dict(),\n",
    "    'diagnostic_history': diagnostic_history,\n",
    "}, f'{cf.logging.run_dir}/final_models.pt')\n",
    "\n",
    "print(f'\\nGAN training complete! Models saved to {cf.logging.run_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4934309d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hinge loss test. --> not working\n",
    "\n",
    "\n",
    "\n",
    "# label smoothing test --> geht, aber disc. stays same\n",
    "\n",
    "# no noise on disc input: --> same\n",
    "\n",
    "# random sigma -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a680b1e2",
   "metadata": {},
   "source": [
    "### Plot Diagnostic History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cfab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_diagnostic_history(diagnostic_history, cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a66176c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9118cff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab1efff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-benchmark-spategan (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
